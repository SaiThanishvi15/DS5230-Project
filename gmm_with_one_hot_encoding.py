# -*- coding: utf-8 -*-
"""GMM with one hot encoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F_9rwd-f8LoWmywDXwqlSVAVP4tOQPxx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from random import uniform
from scipy.special import logsumexp
from sklearn.cluster import KMeans
from sklearn.metrics import roc_auc_score

dataset = pd.read_csv("creditcard.csv")
dataset.head()

class G:
     def __init__(self, mean, s):
        self.mean = mean
        self.s = s
     def p(self, d):
        a = (d - self.mean) / abs(self.s)
        b = (1 / (np.sqrt(2 * np.pi) * abs(self.s))) * np.exp(-a * a / 2)
        return b
     def p_log(self, d):
        a = (d - self.mean) / abs(self.s)
        b = np.log((1 / (np.sqrt(2 * np.pi) * abs(self.s)))) + (-a * a / 2)
        return b
     def np_log(self, x):
        y = (x - self.mean) / abs(self.s)
        y = np.log((1 / (np.sqrt(2 * np.pi) * abs(self.s)))) + (-y ** 2 / 2)
        return y

# Commented out IPython magic to ensure Python compatibility.
class GM:
    def __init__(self, n_components):
        self.g = None
        self.mi = None
        self.n = n_components
        self.t = 0.001
        self.nit = None
        self.ip = 'kmeans'
    def _init_par(self, x, random_state=42):
        samp_n, _ = x.shape
        if self.ip == 'kmeans':
            r = np.zeros((samp_n, self.n))
            l = KMeans(n_clusters=self.n, n_init=1,
                                random_state=random_state).fit(x).labels_
            r[np.arange(samp_n), l] = 1
        else:
            raise ValueError("Unimplemented initialization method '%s'"
#                              % self.ip)
        self.m(x, r)
    def init_m(self, x):
        min_m = min(x)
        max_m = max(x)
        min_s = 1
        max_s = 1
        g = []
        mi = []
        for j in range(self.n):
            g.append(G(uniform(min_m, max_m), uniform(min_s, max_s)))
            mi.append(1 / self.n)
        self.g = g
        self.mi = mi
        return self
    def welp(self, x):
        wlp = []
        for j in range(len(self.g)):
            u = [self.g[j].p_log(u) + np.log(self.mi[j]) for i in x]
            assert len(u) == len(x), 'length of array a error'
            wlp.append(u)
        wlp = np.array(wlp)
        return wlp.T
    def welp_np(self, x):
        x = np.array(x)
        x = x.flatten()
        wlp = []
        for j in range(len(self.g)):
            u = self.g[j].np_log(x)
            assert len(u) == len(x), 'length of array a error'
            wlp.append(u)
        wlp = np.array(wlp)
        return wlp.T  
    def relp(self, x):
        wlp = self.welp_np(x)
        n_log = logsumexp(wlp, axis=1)
        assert len(n_log) == len(x), 'length of n_log error'
        with np.errstate(under='ignore'):
            r_log = wlp - n_log[:, np.newaxis]
        return n_log, r_log
    def e(self, x):
        assert x is not None and len(x) > 0, 'u is none or empty'
        assert self.g is not None and len(self.g) > 0, 'g is none or empty'
        assert self.mi is not None and len(self.mi) > 0, 'm is none or empty'
        assert len(self.g) == len(self.mi), 'length of g and m is not equal'
        n_log, r_log = self.relp(x)
        return np.mean(n_log), r_log
    def m(self, x, r):
        kn = r.sum(axis=0) + 10 * np.finfo(r.dtype).eps
        r = r.T
        for j in range(len(self.g)):
            self.g[j].mean = np.dot(r[j], np.array(x)) / kn[j]
        for j in range(len(self.g)):
            self.g[j].s = np.sqrt(np.dot(r[j], (np.array(x) - self.g[j].mean) ** 2) / kn[j])
        for j in range(len(self.g)):
            self.mi[j] = kn[j] / len(x)
    def f(self, x, max_iter):
        self._init_par(x)
        lb = None
        for j in range(max_iter):
            self.nit = j
            plb = lb
            n_log, r_log = self.e(x)
            self.m(x, np.exp(r_log))
            lb = n_log
            if plb is not None:
                ch = lb - plb
                if abs(ch) < self.t:
                    break
    def p(self, x):
        c = 0
        for j in range(len(self.g)):
            c += self.g[j].p(x) * self.mi[j]
        return c
    def sc(self, x):
        wlp = self.welp_np(x)
        n_log = logsumexp(wlp, axis=1)
        assert len(n_log) == len(x), 'length of n_log error'
        return n_log

class GaussianMixtureModel:
    def __init__(self):
        self.gl = None
        self.lp = None
    def f(self, x, h, n_components, max_iter=100):
        self.lp = np.log(np.bincount(h) / len(h))
        sh = (len(self.lp), x.shape[1])
        self.gl = np.empty(sh, dtype=object)
        for j in range(sh[0]):
            for k in range(sh[1]):
                print('fit model ({0},{1})'.format(j, k))
                mod = GM(n_components)
                u = x[h == j, k:k + 1]
                mod.init_m(u)
                mod.f(u, max_iter)
                self.gl[j, k] = mod
                print('n_iter_: {0}'.format(mod.nit))
    def p_pred(self, x):
        assert self.gl is not None, 'gmm list is none'
        assert self.lp is not None, 'log prior is none'
        sh = (len(self.lp), x.shape[1], x.shape[0])
        o = [[self.gl[j][k].sc(x[:, k:k + 1])
                    for k in range(sh[1])]
                    for j in range(sh[0])]
        ll = np.sum(o, axis=1).T
        lj = self.lp + ll
        pred = np.exp(lj - logsumexp(lj, axis=1, keepdims=True))
        return pred

#shuffled data:
s_data = dataset.sample(frac=1) 
#data with one-hot encoding:
o_data = pd.get_dummies(s_data.columns.tolist()) 
# data after normalization:
n_data = (o_data - o_data.min()) / (o_data.max() - o_data.min()) 
t = "Class"
c = [i for i in n_data if i not in ["Class", "Time", "Amount"]]
inp = dataset[c]
out = dataset[t]     
r = inp.shape[0]
train_inp = inp[:int(r*0.8)]
test_inp = inp[int(r*0.8):]
train_out = out[:int(r*0.8)]
test_out = out[int(r*0.8):]

n_components = 10
g = GaussianMixtureModel()
g.f(np.array(train_inp), np.array(train_out), n_components)

pred = g.p_pred(np.array(test_inp))
accuracy = roc_auc_score(test_out, pred[:, 1])
print("Accuracy of GMM with one hot encoding:",accuracy)